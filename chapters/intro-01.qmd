---
title: "Teoría básica de la probabilidad (*Basic Probability Theory*)"
format: html
---

## Introducción

La teoría de la probabilidad es fundamental para la economía y la econometría. La probabilidad es el lenguaje matemático utilizado para manejar la incertidumbre, lo cual es central para la teoría económica moderna. La probabilidad es también la base de la estadística matemática, que es la base de la teoría econométrica. Entonces, es inevitable arrancar por aquí si se quiere tener un dominio consciente y robusto sobre la econometría. 

En primer lugar, la probabilidad se usa para modelar incertidumbre, variabilidad y aleatoriedad. Es decir, para representar científicamente algo que no sabemos exactamente cuál es su comportamiento (ni cuál será en el futuro). Así, cuando se dice que algo es *incierto* se dice que el resultado es desconocido. Por ejemplo, ¿cuántos estudiantes habrá en la cohorte del pregrado de Sociología de alguna universidad? Por *variabilidad* queremos decir que el resultado no es el mismo en todas las ocurrencias. Por ejemplo, el número de estudiantes del pregrado de Sociología fluctúa de un año a otro, es porque *varía*, posee el atributo de la *variabilidad*. Finalmente, por *aleatoriedad* se entiende que la variabilidad del fenómeno a estudiar tiene algún tipo de *patrón*. Por ejemplo, el número de estudiantes de Sociología puede fluctuar entre 20 y 30, siendo 25 más probable que 20 o 30. Pero es muy improbable que lleguen 100 el próximo año. Con todo ello, la probabilidad nos proporciona un lenguaje matemático para describir incertidumbre, variabilidad y aleatoriedad.

## Resultados (*Outcomes*) y Eventos

Partamos con un ejemplo, supongamos que tomamos una moneda, la lanzamos al aire y la dejamos caer al suelo. ¿Qué ocurrirá? ¿Será el resultado “cara” (H) o “cruz” (T)? No conocemos el resultado por adelantado, por lo que describimos el resultado como aleatorio. Supongamos ahora que se registra el cambio en el valor de un índice bursátil durante un período de tiempo. ¿Aumentará o disminuirá el valor? Nuevamente, no conocemos el resultado por adelantado, por lo que describimos el resultado como aleatorio. O supongamos que seleccionamos un individuo al azar y le preguntas acerca de su situación económica. ¿Cuál es su salario por hora? No lo sabemos por adelantado. La falta de conocimiento previo nos lleva a describir el resultado como aleatorio.

Para todo ello, usaremos los siguientes términos:

  - Un **resultado** (*outcome*) es un resultado específico. Por ejemplo, en un lanzamiento de moneda el resultado es $H$ o $T$. Si dos monedas se lanzan en secuencia, podríamos tener un resultado como $HT$ para “cara” en el primer lanzamiento y “cruz” en el segundo. En el lanzamiento de un dado de seis caras, los resultados posibles son $\{1,2,3,4,5,6\}$.

  - El **espacio muestral** (*sample space*) $S$ es el **conjunto** de todos los resultados posibles. En un lanzamiento de moneda el espacio muestral es $S = \{H, T\}$. Si se lanzan dos monedas, el espacio muestral es $S = \{HH, HT, TH, TT\}$.

  - Un **evento** (*event*) $A$ es un **subconjunto** de resultados en $S$. Un ejemplo proveniente del lanzamiento de un dado es $A = \{1,2\}$.

Los espacios muestrales de una y dos monedas se ilustran en la Figura 1.1. El evento $\{HH, HT\}$ se muestra con la elipse en la @fig-1-espacio-muestral. Las operaciones con conjuntos son útiles para describir eventos. En R, podríamos representarlo así: 

```{r}
#| label: fig-1-espacio-muestral
#| fig.cap: "Espacio muestral"
#| fig.align: "center"
par(mfrow = c(1,2), mar = c(1,1,1,1))

# --- (a) One Coin ---
plot.new()
plot.window(xlim = c(0, 10), ylim = c(0, 10))

# Dibujar el óvalo
symbols(5, 5, circles = 4.5, inches = FALSE, add = TRUE, fg = "black")

# Poner H y T
text(4, 6, "H")
text(6, 4, "T")

title("(a) One Coin")

# --- (b) Two Coins ---
plot.new()
plot.window(xlim = c(0, 10), ylim = c(0, 10))

# Óvalo exterior
symbols(5, 5, circles = 4.5, inches = FALSE, add = TRUE, fg = "black")

# Óvalo interior (subconjunto)
symbols(5, 5, circles = 2.2, inches = FALSE, add = TRUE, fg = "black")

# Etiquetas
text(5, 7, "TH")
text(7, 6.6, "TT")
text(5, 5.8, "HT")
text(5, 4.2, "HH")

title("(b) Two Coins")
```

Para describir las representaciones y operaciones con eventos, usaremos las siguientes definiciones: 

::: {#def-events}

## Conjuntos de eventos

Sean $A$ y $B$ eventos.

1. $A$ es un **subconjunto** de $B$, escrito $A \subseteq B$, si cada elemento de $A$ es un elemento de $B$.

2. El evento sin resultados $\varnothing = \{\}$ se llama **conjunto nulo** (*null*) o **conjunto vacío** (*empty set*).

3. **La unión** $A \cup B$ es el conjunto de todos los resultados que están en $A$ o en $B$ (o en ambos).

4. **La intersección** $A \cap B$ es el conjunto de elementos que están tanto en $A$ como en $B$.

5. **El complemento** $A^{c}$ de $A$ es el conjunto de todos los resultados en $S$ que no están en $A$.

6. **Los eventos $A$ y $B$ son disjuntos** si no tienen resultados en común. Esto es,  
   $$A \cap B = \varnothing.$$

7. Los eventos $A_1, A_2, \ldots$ forman **una partición** de $S$ si son mutuamente disjuntos y su unión es $S$.

:::

Así, @hansen2021intro menciona que los eventos satisfacen las reglas de las operaciones con conjuntos, incluidas las leyes conmutativa, asociativa y distributiva. Para lo cual, el siguiente teorema es útil.

::: {#thm-partitioning}

## Teorema de partición

Si $\{B_1, B_2, \ldots\}$ es una partición de $S$, entonces para cualquier evento $A$ se cumple:

$$
A = \bigcup_{i=1}^{\infty} (A \cap B_i).
$$

Los conjuntos $(A \cap B_i)$ son mutuamente disjuntos^[Una demostración se proporciona en la Sección 2.15.].



:::



## Función de probabilidad

::: {#def-probability-func}

## Función de probabilidad

Una función $\mathbb{P}$ que asigna un valor numérico a los eventosse llama **función de probabilidad** si satisface los siguientes **Axiomas de la Probabilidad**:

1. $\mathbb{P}[A] \ge 0$.

2. $\mathbb{P}[S] = 1$.

3. Si $A_1, A_2, \ldots$ son disjuntos, entonces  
   $$
   \mathbb{P}\!\left[ \bigcup_{j=1}^{\infty} A_j \right]
   = \sum_{j=1}^{\infty} \mathbb{P}[A_j].
   $$

Hansen, utiliza la notación $\mathbb{P}[A]$ o para indicar la probabilidad de un evento $A$. Otra notación común incluye $P(A)$ y $\Pr(A)$ y las usaremos también.

:::


Desglosemos la definición. La frase “una función $\mathbb{P}$ que asigna un valor numérico a los eventos” significa que $\mathbb{P}$ es una función cuyo dominio es el conjunto de eventos y cuyo codominio es la recta real. Por lo tanto, las probabilidades son números. Por lo tanto, ese *outcome*, que es un número, es el *valor* de la probabilidad calculada. Consideremos, además, los Axiomas en cuestión. 

  - El **primer Axioma** establece que las probabilidades son no negativas.

  - El **segundo Axioma** es esencialmente una normalización: la probabilidad de que “algo ocurra” es uno.
  
  - El **tercer Axioma** impone una estructura considerable. Establece que las probabilidades son aditivas en eventos disjuntos. Es decir, si $A$ y $B$ son disjuntos entonces:
  
  $$
  \mathbb{P}[A \cup B] = \mathbb{P}[A] + \mathbb{P}[B].
  $$

Por ejmplo, si tomamos el lanzamiento de un dado, con 6 caras y por tanto 6 resultados posibles, tal que $\{1, 2, 3, 4, 5, 6\}$. Entonces, si los *outcomes* son mutuamente disjuntos, siguiendo el tercer axioma, se tendría que, e.g.,  $\Pr[1 \text{ o } 2] = \Pr(1) + \Pr(2)$. Que, como podríamos intuir sabiendo la probabilidad de cada resultados de un dado, es $\frac{1}{6}+\frac{1}{6}$. Es decir, $\frac{1}{3}=0.\bar{3}$. Simulemos esto en R, lanzando un dado, por ejemplo, 10.000 veces. Debería darnos más o menos 0,3 periodico^[Y si fuera un número cada vez mayor sería cada vez más cercano el valor a 0,33333... Luego veremos por qué]. Supondremos además de que es un dado justo y que tiene reemplazo: 

```{r}
#| label: simulacion-dado-justo
# Simular 10000 lanzamientos de un dado justo
x <- sample(1:6, size = 10000, replace = TRUE)

# Estimar P(1) + P(2)
p1 <- mean(x == 1)
p2 <- mean(x == 2)
p1 + p2


```


Ahora bien, cuando se utiliza el tercer axioma es importante tener cuidado de aplicarlo únicamente a eventos disjuntos. Consideremos, e.g., el lanzamiento de un par de dados. Sea $A$ el evento “1 en el primer lanzamiento” y $B$ el evento “1 en el segundo lanzamiento”. Es tentador escribir  
$$
\mathbb{P}[\text{“1 en alguno de los dos lanzamientos”}] = \mathbb{P}[A \cup B] = \mathbb{P}[A] + \mathbb{P}[B],
$$
pero la segunda igualdad es incorrecta porque $A$ y $B$ no son disjuntos. El resultado “1 en ambos lanzamientos” es un elemento tanto de $A$ como de $B$.

Por otro lado, cualquier función $\mathbb{P}$ que satisfaga los axiomas es una función de probabilidad válida. Consideremos de nuevo el ejemplo del lanzamiento de una moneda. Una función de probabilidad válida fija $\mathbb{P}[H] = 0.5$ y $\mathbb{P}[T] = 0.5$. (Esto se llama típicamente una **moneda equilibrada u honesta** (*fair coin*)). Una segunda función de probabilidad válida fija $\mathbb{P}[H] = 0.6$ y $\mathbb{P}[T] = 0.4$. Sin embargo, una función que fija $\mathbb{P}[H] = -0.6$ no es válida (viola el primer axioma, pues es negativa), y una función que fija $\mathbb{P}[H] = 0.6$ y $\mathbb{P}[T] = 0.6$ no es válida (viola el segundo axioma, ya que $0.6+0.6>1$).

Con todo, aunque la definición establece que una función de probabilidad debe satisfacer ciertas reglas, no describe el **significado** de la probabilidad. La razón es que existen múltiples interpretaciones. Una visión es que las probabilidades son la frecuencia relativa de los resultados en un experimento controlado. La probabilidad de que el mercado bursátil aumente es la frecuencia de aumentos. La probabilidad de que la duración del desempleo exceda un mes es la frecuencia de duraciones de desempleo que exceden un mes. La probabilidad de que un jugador de baloncesto enceste un tiro libre es la frecuencia con la cual el jugador encesta tiros libres. La probabilidad de que ocurra una recesión es la frecuencia relativa de recesiones. Estos ejemplos son, justamente, interpretacionees frecuentistas (relativa en este caso) de la probabilidad.

En algunos ejemplos esto es conceptualmente claro, ya que el experimento se repite o tiene múltiples ocurrencias. En otros casos una situación ocurre exactamente una vez y nunca se repetirá. Al escribir este párrafo, preguntas de incertidumbre de interés general incluyen “¿Saldrá el Reino Unido de la Unión Europea?” y “¿El calentamiento global excederá los 2 grados?”. En estos casos es difícil interpretar la probabilidad como una frecuencia relativa porque el resultado solo puede ocurrir una vez. La interpretación puede rescatarse considerando la “frecuencia relativa” de manera abstracta imaginando muchos universos alternativos que comienzan desde las mismas condiciones iniciales pero evolucionan aleatoriamente.

Otra visión es que la probabilidad es subjetiva. Esta visión sostiene que las probabilidades pueden interpretarse como grados de creencia. Si digo “La probabilidad de que llueva mañana es 80%” quiero decir que esta es mi evaluación subjetiva de la *verosimilitud* basada en la información disponible para mí. Esto puede parecer demasiado amplio ya que permitiría creencias arbitrarias, pero la interpretación subjetiva requiere que la probabilidad subjetiva siga los axiomas y reglas de la probabilidad, y que se actualicen las creencias conforme se revela nueva información^[Es más, la interpretación subjetiva de la probabilidad está en los fundamentos de la teoría bayesiana de la probabilidad]. 

En cualquier caso, lo que tienen en común ambas definiciones es que la función de probabilidad sigue los mismos axiomas; de lo contrario, no debería utilizarse la etiqueta «probabilidad».
















